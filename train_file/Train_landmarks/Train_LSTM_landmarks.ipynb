{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data.iloc[index].values\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1x0</th>\n",
       "      <th>1y0</th>\n",
       "      <th>1z0</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1y1</th>\n",
       "      <th>1z1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1y2</th>\n",
       "      <th>1z2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>...</th>\n",
       "      <th>2z18</th>\n",
       "      <th>2x19</th>\n",
       "      <th>2y19</th>\n",
       "      <th>2z19</th>\n",
       "      <th>2x20</th>\n",
       "      <th>2y20</th>\n",
       "      <th>2z20</th>\n",
       "      <th>frame</th>\n",
       "      <th>sign</th>\n",
       "      <th>sign_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.846833</td>\n",
       "      <td>0.902750</td>\n",
       "      <td>0.770960</td>\n",
       "      <td>0.911748</td>\n",
       "      <td>0.788044</td>\n",
       "      <td>0.827786</td>\n",
       "      <td>0.939425</td>\n",
       "      <td>0.605885</td>\n",
       "      <td>0.809260</td>\n",
       "      <td>0.953103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>ALL DONE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.848541</td>\n",
       "      <td>0.922108</td>\n",
       "      <td>0.770960</td>\n",
       "      <td>0.918250</td>\n",
       "      <td>0.795534</td>\n",
       "      <td>0.855749</td>\n",
       "      <td>0.945106</td>\n",
       "      <td>0.616240</td>\n",
       "      <td>0.858329</td>\n",
       "      <td>0.955665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>ALL DONE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.840546</td>\n",
       "      <td>0.927690</td>\n",
       "      <td>0.770959</td>\n",
       "      <td>0.916599</td>\n",
       "      <td>0.801719</td>\n",
       "      <td>0.829665</td>\n",
       "      <td>0.945177</td>\n",
       "      <td>0.622401</td>\n",
       "      <td>0.815006</td>\n",
       "      <td>0.954720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>ALL DONE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.852944</td>\n",
       "      <td>0.927621</td>\n",
       "      <td>0.770959</td>\n",
       "      <td>0.925096</td>\n",
       "      <td>0.798189</td>\n",
       "      <td>0.874358</td>\n",
       "      <td>0.951644</td>\n",
       "      <td>0.615641</td>\n",
       "      <td>0.882257</td>\n",
       "      <td>0.962133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>ALL DONE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.853046</td>\n",
       "      <td>0.926141</td>\n",
       "      <td>0.770959</td>\n",
       "      <td>0.926612</td>\n",
       "      <td>0.796678</td>\n",
       "      <td>0.884561</td>\n",
       "      <td>0.953647</td>\n",
       "      <td>0.620248</td>\n",
       "      <td>0.906087</td>\n",
       "      <td>0.964290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>ALL DONE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>0.632007</td>\n",
       "      <td>0.666757</td>\n",
       "      <td>0.948843</td>\n",
       "      <td>0.695869</td>\n",
       "      <td>0.635929</td>\n",
       "      <td>0.866754</td>\n",
       "      <td>0.749750</td>\n",
       "      <td>0.589213</td>\n",
       "      <td>0.783786</td>\n",
       "      <td>0.774926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108612</td>\n",
       "      <td>0.679318</td>\n",
       "      <td>0.937836</td>\n",
       "      <td>-0.117675</td>\n",
       "      <td>0.686186</td>\n",
       "      <td>0.891822</td>\n",
       "      <td>-0.124076</td>\n",
       "      <td>15</td>\n",
       "      <td>THANK YOU</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>0.548870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.948842</td>\n",
       "      <td>0.608107</td>\n",
       "      <td>0.870254</td>\n",
       "      <td>0.938450</td>\n",
       "      <td>0.653685</td>\n",
       "      <td>0.722469</td>\n",
       "      <td>0.911040</td>\n",
       "      <td>0.650712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>THANK YOU</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>0.567080</td>\n",
       "      <td>0.790549</td>\n",
       "      <td>0.948843</td>\n",
       "      <td>0.643765</td>\n",
       "      <td>0.700862</td>\n",
       "      <td>0.869206</td>\n",
       "      <td>0.702911</td>\n",
       "      <td>0.583898</td>\n",
       "      <td>0.770457</td>\n",
       "      <td>0.723818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>THANK YOU</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>0.552547</td>\n",
       "      <td>0.850631</td>\n",
       "      <td>0.948842</td>\n",
       "      <td>0.641923</td>\n",
       "      <td>0.782509</td>\n",
       "      <td>0.929966</td>\n",
       "      <td>0.699915</td>\n",
       "      <td>0.672363</td>\n",
       "      <td>0.866254</td>\n",
       "      <td>0.726431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>THANK YOU</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>0.551923</td>\n",
       "      <td>0.871789</td>\n",
       "      <td>0.948843</td>\n",
       "      <td>0.638372</td>\n",
       "      <td>0.795712</td>\n",
       "      <td>0.928676</td>\n",
       "      <td>0.694955</td>\n",
       "      <td>0.678601</td>\n",
       "      <td>0.872127</td>\n",
       "      <td>0.714027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>THANK YOU</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1x0       1y0       1z0       1x1       1y1       1z1       1x2  \\\n",
       "0     0.846833  0.902750  0.770960  0.911748  0.788044  0.827786  0.939425   \n",
       "1     0.848541  0.922108  0.770960  0.918250  0.795534  0.855749  0.945106   \n",
       "2     0.840546  0.927690  0.770959  0.916599  0.801719  0.829665  0.945177   \n",
       "3     0.852944  0.927621  0.770959  0.925096  0.798189  0.874358  0.951644   \n",
       "4     0.853046  0.926141  0.770959  0.926612  0.796678  0.884561  0.953647   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1195  0.632007  0.666757  0.948843  0.695869  0.635929  0.866754  0.749750   \n",
       "1196  0.548870  1.000000  0.948842  0.608107  0.870254  0.938450  0.653685   \n",
       "1197  0.567080  0.790549  0.948843  0.643765  0.700862  0.869206  0.702911   \n",
       "1198  0.552547  0.850631  0.948842  0.641923  0.782509  0.929966  0.699915   \n",
       "1199  0.551923  0.871789  0.948843  0.638372  0.795712  0.928676  0.694955   \n",
       "\n",
       "           1y2       1z2       1x3  ...      2z18      2x19      2y19  \\\n",
       "0     0.605885  0.809260  0.953103  ...  0.000000  0.000000  0.000000   \n",
       "1     0.616240  0.858329  0.955665  ...  0.000000  0.000000  0.000000   \n",
       "2     0.622401  0.815006  0.954720  ...  0.000000  0.000000  0.000000   \n",
       "3     0.615641  0.882257  0.962133  ...  0.000000  0.000000  0.000000   \n",
       "4     0.620248  0.906087  0.964290  ...  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1195  0.589213  0.783786  0.774926  ... -0.108612  0.679318  0.937836   \n",
       "1196  0.722469  0.911040  0.650712  ...  0.000000  0.000000  0.000000   \n",
       "1197  0.583898  0.770457  0.723818  ...  0.000000  0.000000  0.000000   \n",
       "1198  0.672363  0.866254  0.726431  ...  0.000000  0.000000  0.000000   \n",
       "1199  0.678601  0.872127  0.714027  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "          2z19      2x20      2y20      2z20  frame       sign  sign_n  \n",
       "0     0.000000  0.000000  0.000000  0.000000     15   ALL DONE       0  \n",
       "1     0.000000  0.000000  0.000000  0.000000     15   ALL DONE       0  \n",
       "2     0.000000  0.000000  0.000000  0.000000     15   ALL DONE       0  \n",
       "3     0.000000  0.000000  0.000000  0.000000     15   ALL DONE       0  \n",
       "4     0.000000  0.000000  0.000000  0.000000     15   ALL DONE       0  \n",
       "...        ...       ...       ...       ...    ...        ...     ...  \n",
       "1195 -0.117675  0.686186  0.891822 -0.124076     15  THANK YOU       2  \n",
       "1196  0.000000  0.000000  0.000000  0.000000     15  THANK YOU       2  \n",
       "1197  0.000000  0.000000  0.000000  0.000000     15  THANK YOU       2  \n",
       "1198  0.000000  0.000000  0.000000  0.000000     15  THANK YOU       2  \n",
       "1199  0.000000  0.000000  0.000000  0.000000     15  THANK YOU       2  \n",
       "\n",
       "[1200 rows x 129 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"D:\\\\project\\\\dataset\\\\gen_land_marks_dataset\\\\FrameLM.csv\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "data['sign_n'] = data['sign'].astype('category').cat.codes\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypints_array = data.values[:,:126]#.reshape(80, 15, 126)\n",
    "lable_array = data.values[:,-1].reshape(80, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypints_array = keypints_array.reshape(80, 15, 126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_array_ = np.zeros(80)\n",
    "for i in range(lable_array_.shape[0]):\n",
    "    lable_array_[i] = lable_array[i][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lable_array_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 15, 126)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypints_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDatasetLSTM(Dataset):\n",
    "    def __init__(self, data, lable):\n",
    "        self.data = data.astype(np.float64)\n",
    "        self.lable = lable\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print(self.data[index])\n",
    "        return torch.tensor( self.data[index]).float(), self.lable[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "dataset = MyDatasetLSTM(keypints_array, lable_array_)\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#val_size = len(dataset) - train_size\n",
    "#t , v = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.8468,  0.9027,  0.7710,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.8485,  0.9221,  0.7710,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.8405,  0.9277,  0.7710,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0989,  0.9731,  0.7710,  ...,  0.9065,  0.5294, -0.0606],\n",
       "         [ 0.1095,  0.9907,  0.7710,  ...,  0.9556,  0.5438, -0.0664],\n",
       "         [ 0.1116,  1.0000,  0.7710,  ...,  0.9860,  0.5429, -0.0835]]),\n",
       " 0.0)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset[0]\n",
    "lable_array_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test_dataset = MyDataset(test_data)\n",
    "TrainDataLoader = DataLoader(dataset, batch_size=1)\n",
    "#TestDataLoader = DataLoader(v, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMmodel(nn.Module):\n",
    "    def __init__(self, num_classes ):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=126, hidden_size=500, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(500, num_classes)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model = LSTMmodel(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"D:/project/asl/checkpoint_lm_LSTM.pt\"\n",
    "\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to(device)\n",
    "opt_myCNN = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    #print(\"validate\")\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_correct = 0\n",
    "    val_running_errors = 0\n",
    "    #print(\"1\")\n",
    "    with torch.no_grad():\n",
    "        #print(\"2\")\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(dataloader.dataset)/dataloader.batch_size)):\n",
    "            #print(data[1].shape)\n",
    "            data, labels = data[0].to(device), data[1].to(device)\n",
    "            data\n",
    "            outputs = model(data)\n",
    "            #print(outputs)\n",
    "            #print(labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item()* data.size(0)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            val_running_correct += torch.sum(preds == labels)\n",
    "            val_running_errors += torch.sum(preds != labels)\n",
    "\n",
    "        val_loss = val_running_loss/len(dataloader.dataset)\n",
    "        val_accuracy = val_running_correct/len(dataloader.dataset)\n",
    "        val_errore = val_running_errors/len(dataloader.dataset)\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy}')\n",
    "        \n",
    "        return val_loss, val_accuracy, val_errore\n",
    "# train part\n",
    "def train(model,optimizer, dataloader):\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    train_running_errors = 0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(dataloader.dataset) /dataloader.batch_size)):\n",
    "\n",
    "        data, labels = data[0].to(device), data[1].to(device)\n",
    "        data =  torch.reshape(data, (15, 1, 126))\n",
    "        print(data.shape)\n",
    "        print(labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        print(outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_errors += torch.sum(preds != labels)\n",
    "        train_running_correct += torch.sum(preds == labels)\n",
    "        train_running_loss += loss.item()* data.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = train_running_loss/len(dataloader.dataset)\n",
    "    train_accuracy = train_running_correct/len(dataloader.dataset)\n",
    "    train_errore = train_running_errors/len(dataloader.dataset)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy}\")\n",
    "    return train_loss, train_accuracy, train_errore\n",
    " #  main \n",
    "def model_train(model, optimizer, train_dataloader, epochs):\n",
    "  err_history = {\"train\" : [], \"val\" : []}\n",
    "  acc_history =  {\"train\" : [], \"val\" : []}\n",
    "  loss_history = {\"train\" : [], \"val\" : []}\n",
    "  for epoch in range(epochs):\n",
    "      print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "      phase = \"train\"\n",
    "      print(phase)\n",
    "      loss, accuracy, error = train(model, optimizer, train_dataloader)\n",
    "      #phase = \"val\"\n",
    "      #print(phase)\n",
    "      #loss, accuracy, error = validate(model, test_dataloader)\n",
    "\n",
    "  save_model(model)\n",
    "  return err_history, acc_history, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 150\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 1, 126])\n",
      "torch.Size([1])\n",
      "torch.Size([15, 1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (15) to match target batch_size (1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m myCNN_err_history, myCNN_acc_history , myCNN_loss_history \u001b[39m=\u001b[39m model_train(model, opt_myCNN, TrainDataLoader ,num_epochs )\n",
      "Cell \u001b[1;32mIn[191], line 69\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(model, optimizer, train_dataloader, epochs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     phase \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m     \u001b[39mprint\u001b[39m(phase)\n\u001b[1;32m---> 69\u001b[0m     loss, accuracy, error \u001b[39m=\u001b[39m train(model, optimizer, train_dataloader)\n\u001b[0;32m     70\u001b[0m     \u001b[39m#phase = \"val\"\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[39m#print(phase)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[39m#loss, accuracy, error = validate(model, test_dataloader)\u001b[39;00m\n\u001b[0;32m     74\u001b[0m save_model(model)\n",
      "Cell \u001b[1;32mIn[191], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, dataloader)\u001b[0m\n\u001b[0;32m     43\u001b[0m outputs \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     44\u001b[0m \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 45\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     47\u001b[0m _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m train_running_errors \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(preds \u001b[39m!=\u001b[39m labels)\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (15) to match target batch_size (1)."
     ]
    }
   ],
   "source": [
    "myCNN_err_history, myCNN_acc_history , myCNN_loss_history = model_train(model, opt_myCNN, TrainDataLoader ,num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m val_running_errors \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39m#with torch.no_grad():\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m TestDataLoader:\n\u001b[0;32m      8\u001b[0m     \u001b[39m#print(data[0].shape)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     data, labels \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m     outputs \u001b[39m=\u001b[39m model(data)\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3631\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3632\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3633\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3634\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "print(\"1\")\n",
    "val_running_loss = 0.0\n",
    "val_running_correct = 0\n",
    "val_running_errors = 0\n",
    "#with torch.no_grad():\n",
    "for data in TestDataLoader:\n",
    "    #print(data[0].shape)\n",
    "    data, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "    outputs = model(data)\n",
    "    #print(labels)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    val_running_loss += loss.item()* data.size(0)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    #print(preds)\n",
    "    val_running_correct += torch.sum(preds == labels)\n",
    "    val_running_errors += torch.sum(preds != labels)\n",
    "\n",
    "val_loss = val_running_loss/len(TestDataLoader.dataset)\n",
    "val_accuracy = val_running_correct/len(TestDataLoader.dataset)\n",
    "print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [0]*29\n",
    "for e in val_loader.dataset:\n",
    "    count[e[1]] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 66.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 8.1486, Val Acc: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs, leble, pred  = validate_(model, _ , test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22, device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, \n",
    "                      normalized=False, \n",
    "                      title=None, \n",
    "                      cmap=plt.cm.Blues,\n",
    "                      size=(16,12)):\n",
    "    fig, ax = plt.subplots(figsize=size)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(pred.to(\"cpu\"), leble.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm,\n",
    "                      normalized=True, \n",
    "                      title=\"Model Performance\", \n",
    "                      cmap=plt.cm.Blues,\n",
    "                      size=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        3, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 2, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 3, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 3, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 4]], dtype=int64)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = SimpleCNN(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=9216, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=29, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_state_dict = torch.load(checkpoint_path)\n",
    "\n",
    "model_test.load_state_dict(saved_state_dict)\n",
    "model_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87000"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.3891, Val Acc: 0.0357142873108387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out, lab, preds =  validate_(model, \"8\" ,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
       "        28, 28, 28, 28, 28, 28, 28, 28, 28, 28], device='cuda:0')"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 15, 14, 16, 17, 18,\n",
       "        19, 21, 20, 22, 23, 24, 25, 26, 27, 28], device='cuda:0')"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[168], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_dataset[:,\u001b[39m0\u001b[39;49m]\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataset.py:196\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(tensor[index] \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensors)\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataset.py:196\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(tensor[index] \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "test_dataset[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_R  =model_test(images_t.to(device))\n",
    "loss = criterion(out_R, lables_t.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  25.1246,  -17.7779,  -30.0222,  -46.3044,  -13.8914,  -13.6885,\n",
       "          -33.4458,  -59.6534,  -17.4975,  -46.3703,  -45.5736,   -0.7717,\n",
       "          -10.1311,  -16.1549,   -4.7489,  -39.6303,  -25.6147,  -51.9624,\n",
       "            8.8614,   27.1093,  -26.5826,  -34.8662,  -16.8425,   10.4190,\n",
       "            0.4968,   -6.2862,  -45.1497,   -7.3668,  -32.7714],\n",
       "        [  -3.4972,   31.2964,  -28.4469,   -3.5423,   19.0190,  -17.2941,\n",
       "          -26.2976,  -15.2658,    5.8858,  -37.4065,  -36.9119,  -33.4379,\n",
       "          -15.3079,  -18.1010,  -20.4883,  -24.0730,  -25.1254,  -32.8994,\n",
       "           -3.7259,  -26.4858,  -11.4578,  -49.4884,  -27.8632,  -28.3417,\n",
       "          -25.3667,  -53.3120,  -26.9595,   -9.6482,  -48.6743],\n",
       "        [ -37.7241,  -78.8876,   92.2101,   29.9073,    7.8852,  -19.0678,\n",
       "          -12.7656,  -54.3811,  -25.9852,  -99.3926, -131.8276,  -40.5023,\n",
       "          -76.2138,  -68.5968,   17.5785,  -41.7020,    5.9910, -155.1064,\n",
       "           -2.1584,  -23.8685, -105.0168, -131.7552, -169.5239,  -63.6790,\n",
       "          -33.7259,   30.3628,   18.0137,  -32.9418,  -46.1787],\n",
       "        [ -43.1520,  -33.1402,   -5.7436,   57.4406,    1.7557,   19.0642,\n",
       "          -22.1134,  -37.9803,    1.8941,  -62.0435,  -65.6781,  -18.7327,\n",
       "          -58.8600,  -51.9612,    9.1509,  -16.7967,  -17.8861,  -85.1006,\n",
       "          -32.6213,  -34.9554,  -63.6837,  -47.0748,  -67.9649,  -39.8730,\n",
       "          -14.0067,  -45.8955,  -17.4450,  -68.1668,  -34.0400],\n",
       "        [  20.2645,   -3.7573,  -31.7393,    7.7063,   38.3035,   12.6444,\n",
       "          -10.7683,  -54.0215,    3.1104,  -60.2857,  -47.1771,  -40.0667,\n",
       "          -27.1068,  -44.0735,    4.6918,  -46.6405,  -24.8986,  -78.0759,\n",
       "          -13.4820,  -15.8677,  -41.8449,  -58.1296,  -65.1022,  -52.7189,\n",
       "          -31.3010,  -25.3980,  -30.9382,  -26.6842,  -79.5053],\n",
       "        [ -22.0952,  -58.9658, -105.9830,  -18.0496,  -29.3041,  124.1692,\n",
       "           -3.5248,  -56.1854,    9.4240,  -25.2544,  -28.9833,  -35.4988,\n",
       "          -70.8722,  -48.1002,   19.5317,    5.3051,  -76.0274, -182.8707,\n",
       "          -76.2867,  -34.2735, -115.1571,  -65.4220,   -6.2840,  -70.9898,\n",
       "           27.2630, -105.3792,  -61.8699, -111.8363,  -30.0586],\n",
       "        [ -45.5425,  -68.9265,  -21.5899,  -46.5346,  -37.3658,  -23.5780,\n",
       "           50.7408,    1.9697,  -24.8994,    9.9693,  -74.0479,  -74.9919,\n",
       "          -14.0402,   -8.9070,  -33.0695,   20.7549,  -14.2521,  -86.5629,\n",
       "          -30.3690,  -52.1317,  -74.5775,  -72.9698,  -76.0749,  -42.2438,\n",
       "           -3.6299,  -76.5106,   -7.8268,  -31.1576,   21.5546],\n",
       "        [ -79.0682,  -60.5479,  -62.9466,  -36.0485,   -3.9023,  -22.2958,\n",
       "           -0.8298,   67.6636,    3.7298,   -2.3941,  -64.3534, -107.1620,\n",
       "          -60.8461,   -4.3149,   -4.6937,   -9.0493,  -33.2198,  -79.1775,\n",
       "          -27.1631,  -46.4314,  -50.6007,  -98.7439,  -93.5880,  -84.1264,\n",
       "          -15.2339,  -74.0611,   -2.5835,  -32.6466,  -23.5821],\n",
       "        [ -17.1750,  -28.7517,  -15.4973,   11.5423,    3.1391,   -9.6922,\n",
       "          -31.5689,   11.4391,   45.2949,  -20.9887,  -25.5823,  -43.8377,\n",
       "          -23.9706,  -26.5642,   -0.2033,  -23.8357,    0.3923,  -34.2096,\n",
       "           -5.0404,   -6.1268,  -51.1531,  -73.9963,  -37.1264,  -30.6386,\n",
       "          -27.7968,  -40.3132,  -13.3891,  -24.0019,  -21.0502],\n",
       "        [ -57.3735,  -77.5826,  -80.2478,  -41.1496,  -32.8216,   -1.7769,\n",
       "           18.5740,   17.4793,    4.0679,   37.1401,  -32.1148,  -75.8709,\n",
       "          -31.6632,  -20.5752,  -25.1517,    1.0556,  -33.9299,  -58.2993,\n",
       "          -34.0643,  -43.5029,  -51.6841,  -63.7677,  -60.3359,  -58.5820,\n",
       "            2.2884,  -84.6368,  -37.4474,  -63.7544,   10.1795],\n",
       "        [ -16.2942,   -7.1062,  -47.9084,   -5.1062,   -9.3271,   -6.8862,\n",
       "          -33.5449,    2.8849,   20.6369,  -13.7747,   25.1132,  -27.9303,\n",
       "          -15.3720,  -12.9233,   -4.6270,  -47.4946,  -43.9679,  -14.9654,\n",
       "          -37.6987,  -18.4538,  -16.1565,  -15.5656,   -2.8320,  -28.5607,\n",
       "          -18.4185,  -25.9136,  -27.5818,  -22.2157,  -15.6174],\n",
       "        [ -14.3960,  -28.6732,    4.4523,   -3.6744,  -23.2675,   -4.5156,\n",
       "          -25.1226,  -63.9155,  -35.8756,  -35.1974,  -35.5191,   33.7056,\n",
       "          -10.6446,  -30.7536,    6.3647,   -5.5987,  -15.0153,  -47.8366,\n",
       "          -16.1412,   -6.2991,  -29.6583,  -26.4652,  -34.9479,  -24.9823,\n",
       "           -8.6046,   11.8569,  -33.0252,  -35.2322,  -23.6163],\n",
       "        [ -19.2906,  -29.0077,  -51.4438,  -40.6730,   -7.1453,  -14.9257,\n",
       "          -22.3207,  -39.2855,  -26.6929,  -13.4920,  -39.1535,  -24.7734,\n",
       "           26.8946,    2.6394,    7.6368,  -10.4370,  -31.9682,  -32.6444,\n",
       "            0.3141,  -11.2440,  -22.5858,  -51.1807,  -32.3045,  -21.1556,\n",
       "          -12.3555,  -42.3637,  -15.4179,  -27.1508,  -21.9223],\n",
       "        [   6.0719,    5.6951,  -15.5015,   -4.6606,    8.5719,  -13.0947,\n",
       "           -6.8226,   -9.8310,   -9.6197,  -20.5780,  -10.7221,  -28.7426,\n",
       "          -11.9582,  -19.1136,  -13.1112,  -12.8741,  -14.7450,  -15.3214,\n",
       "           -5.9018,  -19.9812,   -9.9424,  -24.2552,  -16.2707,  -25.2650,\n",
       "          -27.4515,  -22.5486,  -20.2318,   21.5302,  -27.3902],\n",
       "        [ -28.1322,  -50.4854,  -21.5412,  -39.7145,  -12.1514,  -13.5552,\n",
       "          -29.3294,  -26.8146,  -16.1579,  -22.3861,  -74.5034,  -13.2201,\n",
       "           -2.1288,   46.9998,   -8.9971,   -4.3610,   -5.0214,  -48.3959,\n",
       "           11.7788,   -6.2937,  -46.1846,  -68.5044,  -53.8394,  -21.9598,\n",
       "            5.0506,  -44.8597,   -6.4323,  -36.5095,  -20.4578],\n",
       "        [ -32.7408,  -33.3275,  -18.7237,   19.9147,    5.0788,  -12.7002,\n",
       "          -64.0020,  -61.8851,  -43.4740,  -58.8678,  -72.9089,  -15.7378,\n",
       "          -16.9222,   -4.1685,   51.6844,  -14.4666,  -16.7515,  -80.1632,\n",
       "          -17.0209,  -18.1827,  -32.6830,  -78.1220,  -72.8353,  -60.6894,\n",
       "          -22.2862,   -9.9689,  -19.1631,  -34.0715,  -59.5119],\n",
       "        [ -63.6834, -107.8879,  -25.3204,  -17.4025,  -20.8589,   -1.7443,\n",
       "           -5.0826,   -0.7594,  -35.9012,   -8.5718, -121.8123,  -34.5474,\n",
       "          -46.1184,    7.9195,   -3.3485,   68.3438,   23.4093, -108.2046,\n",
       "          -11.8696,  -30.0791,  -75.0662, -137.1477, -113.8489,  -71.8833,\n",
       "           -7.3156,  -82.3916,   -7.6840,  -54.8829,  -20.6332],\n",
       "        [ -27.1864,  -73.2454,   11.0580,  -20.6611,  -15.7532,  -12.0904,\n",
       "          -22.5368,  -41.7185,  -32.0956,  -35.6986,  -99.2806,  -41.6397,\n",
       "          -51.6123,  -20.6826,    7.9383,   20.8260,   59.0578,  -73.5926,\n",
       "            5.4922,   -1.6204,  -38.6497,  -69.9937,  -98.5748,  -30.7179,\n",
       "           -5.9978,  -24.5115,   27.1883,  -44.1054,  -17.5474],\n",
       "        [ -14.4298,   -5.3820,  -27.8712,  -17.5933,   -7.0497,  -24.2839,\n",
       "          -23.8558,  -24.3535,  -14.2207,  -20.6689,  -19.3727,  -18.2222,\n",
       "           -1.7959,   -9.3706,   -5.3732,  -20.1432,  -12.2809,    5.4448,\n",
       "           -7.4693,   -3.6204,    1.2759,   -9.1172,   -0.6728,   -4.6342,\n",
       "           -5.2756,   -7.2306,   -8.0899,   -7.9206,  -10.3834],\n",
       "        [ -21.0829,  -23.6585,  -16.5260,   -9.7669,  -11.5437,  -10.9918,\n",
       "            0.9763,   -9.0600,  -16.0506,   -2.5822,  -15.1858,  -14.7576,\n",
       "          -10.1440,  -19.3718,   -5.1481,   -3.1229,  -19.5861,  -18.6982,\n",
       "           -6.3665,  -16.5336,  -11.4761,    7.3422,   -7.4035,   -1.5129,\n",
       "            4.7013,  -17.2033,   -6.0179,  -16.3925,   16.1544],\n",
       "        [  -5.9616,  -28.0956,  -28.9238,  -30.1121,   -6.2992,  -17.2761,\n",
       "          -49.8627,  -36.8888,  -12.0309,  -32.1349,  -48.1790,  -20.8967,\n",
       "           -2.2950,    5.7410,    8.1856,  -20.5252,   -6.2073,  -18.2708,\n",
       "           19.0294,   -0.5454,  -10.6122,  -48.9671,  -17.8115,  -14.0310,\n",
       "           -9.1377,   -8.9452,  -17.9852,  -12.9455,  -26.1003],\n",
       "        [   7.8401,  -20.9933,  -12.5248,  -27.0564,  -11.1357,  -15.0853,\n",
       "          -39.9047,  -45.9018,    5.2263,  -22.2339,  -30.4130,    9.3556,\n",
       "           -9.2061,  -25.3434,  -15.4472,  -12.8445,  -14.1466,  -32.1037,\n",
       "           -5.5666,   35.4986,  -31.6320,  -34.2217,  -29.4705,   -3.8114,\n",
       "           -1.5444,    2.3999,  -43.6786,  -10.7614,  -28.3313],\n",
       "        [ -21.6755,  -11.2061,  -28.9631,  -23.1013,  -19.5535,  -37.3936,\n",
       "          -34.7510,  -34.7609,  -25.5521,  -27.6056,  -23.2078,  -13.5926,\n",
       "           -8.6117,  -22.2463,   -3.7951,  -29.3940,  -25.9261,   -8.4689,\n",
       "           -9.0133,   -2.9561,   25.2949,   -7.0398,   -2.3992,   -2.1940,\n",
       "            1.0884,   -2.7693,  -19.0471,  -14.4801,  -16.5319],\n",
       "        [ -12.2738,  -10.3516,  -35.1185,  -13.4895,  -11.7248,  -22.7172,\n",
       "          -14.8711,  -42.4723,  -21.4429,  -30.8705,  -15.8011,  -20.2136,\n",
       "          -10.3583,  -32.8414,    3.0418,  -32.4846,  -41.5494,  -10.4984,\n",
       "          -27.8986,  -13.3063,    0.2465,   20.5789,    1.7366,    0.4661,\n",
       "          -11.1509,    2.4605,   -6.6271,  -16.2542,  -12.0830],\n",
       "        [ -11.3573,  -19.2105,  -66.3038,  -18.0373,  -22.8175,   -3.4086,\n",
       "          -34.8471,  -46.6615,   -7.5405,  -53.8466,   -1.3214,  -35.7771,\n",
       "          -18.5931,  -26.6796,   12.4205,  -40.5705,  -49.6308,  -49.6972,\n",
       "          -47.1923,  -16.4773,  -16.0996,    5.3214,   47.1341,  -30.7488,\n",
       "            4.4268,   -4.7410,  -10.0889,  -41.6665,  -20.5689],\n",
       "        [ -20.0243,  -22.2108,  -23.7158,  -22.1593,   -4.2323,  -23.0276,\n",
       "          -12.6784,  -23.1164,   -5.4489,  -18.0282,  -41.2111,  -13.6399,\n",
       "           -5.5169,  -10.3980,   -6.7172,  -21.7687,  -16.5986,  -13.2476,\n",
       "           -1.4614,   -2.8559,   -3.1864,  -16.1631,  -17.4186,   14.0050,\n",
       "            0.6108,  -11.0217,   -8.1063,   -8.1817,   -6.3883],\n",
       "        [ -28.8606,  -61.3406,  -76.9891,  -35.3036,  -37.1986,   -7.5814,\n",
       "            5.3879,  -53.7138,  -19.9406,   -0.4649,  -20.2584,  -30.8026,\n",
       "          -21.4633,  -25.1135,  -19.8199,  -25.0668,  -44.6510,  -29.2241,\n",
       "          -36.8563,  -12.0913,  -28.3614,   -6.6417,   -3.8381,   -3.9508,\n",
       "           31.2637,  -19.7768,  -28.6823,  -51.7370,   18.3503],\n",
       "        [   5.0342,  -24.3924,  -22.5827,  -11.7993,  -24.2276,  -22.6345,\n",
       "          -28.2361,  -58.5876,  -21.3279,  -45.2104,  -17.2471,  -18.2412,\n",
       "          -11.7160,  -44.0282,    7.2387,  -40.7877,  -31.0205,  -17.9039,\n",
       "          -25.3057,  -15.2152,  -27.5470,   -2.8502,   -4.0985,   -3.7424,\n",
       "          -15.4449,   24.2863,   -8.2497,  -23.0158,  -19.8643]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, preds = torch.max(out_R.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 27.1093,  31.2964,  92.2101,  57.4406,  38.3035, 124.1692,  50.7408,\n",
       "         67.6636,  45.2949,  37.1401,  25.1132,  33.7056,  26.8946,  21.5302,\n",
       "         46.9998,  51.6844,  68.3438,  59.0578,   5.4448,  16.1544,  19.0294,\n",
       "         35.4986,  25.2949,  20.5789,  47.1341,  14.0050,  31.2637,  24.2863],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 27, 13, 14, 15, 16,\n",
       "        17, 28, 18, 19, 20, 21, 22, 23, 24, 25], device='cuda:0')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 15, 14, 16, 17, 18,\n",
       "        19, 21, 20, 22, 23, 24, 25, 26, 27, 28])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lables_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(45.9709, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
